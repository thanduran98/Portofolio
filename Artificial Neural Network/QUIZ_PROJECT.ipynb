{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QUIZ 1 ANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9rygm7I6AiA"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gs_NYpt7DQ5"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7MlqQ2o7F62"
      },
      "source": [
        "dataset = pd.read_csv(\"pulsar_stars.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk-rQf1O94j4"
      },
      "source": [
        "def load_dataset():\n",
        "    dataset = pd.read_csv(\"pulsar_stars.csv\")\n",
        "    feature = dataset[[ \"Mean of the integrated profile\",\n",
        "                        \"Standard deviation of the integrated profile\",\n",
        "                        \"Excess kurtosis of the integrated profile\", \n",
        "                        \"Skewness of the integrated profile\",\n",
        "                        \"Mean of the DM-SNR curve\",\n",
        "                        \"Standard deviation of the DM-SNR curve\",\n",
        "                        \"Excess kurtosis of the DM-SNR curve\",\n",
        "                        \"Skewness of the DM-SNR curve\"]]\n",
        "    target = dataset[[\"target_class\"]]\n",
        "\n",
        "    normalizer = StandardScaler()\n",
        "    feature = normalizer.fit_transform(feature)\n",
        "    OHencoder = OneHotEncoder(sparse=False)\n",
        "    target = OHencoder.fit_transform(target)\n",
        "\n",
        "    return feature, target\n",
        "\n",
        "feature, target = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD2sJh-298uW"
      },
      "source": [
        "layer = {\n",
        "    \"input\" : 8,\n",
        "    \"hidden\" : 4,\n",
        "    \"output\" : 2\n",
        "}\n",
        "\n",
        "weight = {\n",
        "    \"input_hidden\" : tf.Variable(tf.random_normal(\n",
        "        [layer[\"input\"],layer[\"hidden\"]]\n",
        "    )),\n",
        "    \"hidden_output\" : tf.Variable(tf.random_normal(\n",
        "        [layer[\"hidden\"], layer[\"output\"]]\n",
        "    ))\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"input_hidden\" : tf.Variable(tf.random_normal(\n",
        "        [layer[\"hidden\"]]\n",
        "    )),\n",
        "    \"hidden_output\" : tf.Variable(tf.random_normal(\n",
        "        [layer[\"output\"]]\n",
        "    ))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1jj1xfx-INd"
      },
      "source": [
        "input_dataset = tf.placeholder(tf.float32, [None, layer[\"input\"]])\n",
        "output_dataset = tf.placeholder(tf.float32,[None, layer[\"output\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl0kaNpS-adW"
      },
      "source": [
        "def feed_forward(dataset):\n",
        "    u1 = tf.matmul(dataset,weight[\"input_hidden\"]) + bias[\"input_hidden\"]\n",
        "    y1 = tf.nn.sigmoid(u1)\n",
        "\n",
        "    u2 = tf.matmul(y1,weight[\"hidden_output\"]) + bias[\"hidden_output\"]\n",
        "    y2 = tf.nn.sigmoid(u2)\n",
        "\n",
        "    return y2\n",
        "\n",
        "output = feed_forward(input_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOUhwQrB_66C"
      },
      "source": [
        "epoch = 5000\n",
        "alpha = 0.9123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7tpK9YpAXel"
      },
      "source": [
        "#calculate Loss\n",
        "loss = tf.reduce_mean(0.5*(output_dataset - output)**2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI-H_MG6AhHc"
      },
      "source": [
        "#optimize loss\n",
        "optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
        "train = optimizer.minimize(loss)\n",
        "#accuracy\n",
        "accuracy = tf.equal(tf.argmax(output_dataset,axis=1,tf.argmax(output, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(accuracy,tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F37U8HKAlEl"
      },
      "source": [
        "feature_train, feature_test, target_train, target_test = train_test_split(feature, target, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlhDzIBO6lDA",
        "outputId": "32f56d33-110a-4f4d-aca6-2c41a8e59167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    feed_train = {\n",
        "          input_dataset: feature_train,\n",
        "          output_dataset: target_train\n",
        "    }\n",
        "\n",
        "    feed_test = {\n",
        "        input_dataset: feature_test,\n",
        "        output_dataset: target_test\n",
        "    }   \n",
        "\n",
        "    #train\n",
        "    for i in range(epoch):\n",
        "        sess.run(train, feed_dict=feed_train)\n",
        "        error = sess.run(loss, feed_dict=feed_train)\n",
        "\n",
        "        if (i+1)%200 == 0:\n",
        "            print(\"Epoch: {}, Error: {}\".format(i+1,error))\n",
        "\n",
        "    #testing\n",
        "    print(\"Accuracy: {}\\n\".format(sess.run(accuracy*100, feed_dict = feed_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 200, Error: 0.028996704146265984\n",
            "Epoch: 400, Error: 0.019676854833960533\n",
            "Epoch: 600, Error: 0.016172293573617935\n",
            "Epoch: 800, Error: 0.014378368854522705\n",
            "Epoch: 1000, Error: 0.013285813853144646\n",
            "Epoch: 1200, Error: 0.012545849196612835\n",
            "Epoch: 1400, Error: 0.012029727920889854\n",
            "Epoch: 1600, Error: 0.011636667884886265\n",
            "Epoch: 1800, Error: 0.011329754255712032\n",
            "Epoch: 2000, Error: 0.011094887740910053\n",
            "Epoch: 2200, Error: 0.01091318391263485\n",
            "Epoch: 2400, Error: 0.010769831947982311\n",
            "Epoch: 2600, Error: 0.010653979144990444\n",
            "Epoch: 2800, Error: 0.010557967238128185\n",
            "Epoch: 3000, Error: 0.01047660131007433\n",
            "Epoch: 3200, Error: 0.010406364686787128\n",
            "Epoch: 3400, Error: 0.010344810783863068\n",
            "Epoch: 3600, Error: 0.01029018871486187\n",
            "Epoch: 3800, Error: 0.010241221636533737\n",
            "Epoch: 4000, Error: 0.010196929797530174\n",
            "Epoch: 4200, Error: 0.010156575590372086\n",
            "Epoch: 4400, Error: 0.010119556449353695\n",
            "Epoch: 4600, Error: 0.010085408575832844\n",
            "Epoch: 4800, Error: 0.010053747333586216\n",
            "Epoch: 5000, Error: 0.010024260729551315\n",
            "Accuracy: 99.13544464111328\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}